# 卷积神经网络

卷积神经网络（CNN），是前馈神经网络的一种，相比于此前的BP网络、BRF网络等，通过 **卷积** 操作后，可以减少噪声，提高训练结果模型的泛化性，同时能够起到压缩参数，减小模型体积的作用。因此，卷积神经网络天生就比同等情况下的 BP网络 等具有更快收敛速度。  

实际上，卷积神经网络就是在普通神经网络的基础上增加了 **卷积层**（有时也会额外增加 **池化层**）。  

# 1. 卷积

卷积的定义：  

![](http://ogemdlrap.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-02%20%E4%B8%8B%E5%8D%884.06.48.png)  

它的数学含义就是，通过两个函数 f(x) 和 g(x) 生成一个新的函数 h(x) 的一种数学算子（运算，就像加减乘除一样），表征函数 f(x) 和 函数 g(x) 经过翻转、平移的重叠部分面积。   

其物理含义是，一个函数在另一个函数上的 **加权叠加**。  
看下这个解释的比较好的例子：  

已知x[0] = a, x[1] = b, x[2]=c，  

![](http://ogemdlrap.bkt.clouddn.com/153fd3e7911d486edaf0475afb1e54b3_hd.png)  

已知y[0] = i, y[1] = j, y[2]=k

![](http://ogemdlrap.bkt.clouddn.com/c47d9d7f7a29c491782bf7b1baea3f8e_hd.png)  

x[n] * y[n]的过程如下，  

第一步，x[n]乘以y[0]并平移到位置0：  

![](http://ogemdlrap.bkt.clouddn.com/91f5eff235013ac729c44e98b3a537d0_hd.png)  

第二步，x[n]乘以y[1]并平移到位置1：  
![](http://ogemdlrap.bkt.clouddn.com/67c05239b05f671766b9df9393026f2c_r.jpg)


第三步，x[n]乘以y[2]并平移到位置2：  
![](http://ogemdlrap.bkt.clouddn.com/c34e839a49c6b616c57bde3c3dbbd67d_hd.png)

最后，把上面三个图叠加，就得到了x[n] * y[n]：  
![](http://ogemdlrap.bkt.clouddn.com/4ce6cdcc28b10aca73db3f877d86ca02_hd.png)

在进行积分的时候，f(t) 不动，g(-t)函数相当于沿着 y 轴进行了翻转，然后再平移 x 个单位，此时两个函数在一个范围内相重叠的面积部分，就是卷积的结果。往简单了说就是，一个变量a会因为另一个变量b的输入而改变，a持续的受到b的输入而改变，每一次新的改变都是在上一次改变的基础上进行的，即有一个叠加的过程在里面。  

# 2. 卷积核

了解了 **卷积**  的意义后，就不难理解，**卷积核** 其实就是用来提取输入信息的特征的。它的表示表达式为：  

```
f(x) = wx + b
```
在训练过程中，w、b 是被随机初始化的，然后通过迭代算法，一点点收敛至极值附近。  

![](http://ogemdlrap.bkt.clouddn.com/%E5%8D%B7%E7%A7%AF%E5%8A%A8%E7%94%BB.gif)  

如图，输入了一个 5x5 的信息矩阵，然后用一个 3x3 的卷积核进行卷积运算，最终获得一个 3x3 的矩阵，这个矩阵就是从信息矩阵中经过卷积核提取出来的抽象特征信息，它会被储存在这个卷积核的 Feature Map 中。  

在卷积的过程中，我们需要设定好 **步长（stride）** ，如上图中的 `stride = 1` ， 即一次偏移一个单位距离后，进行一次计算。

在卷积核输出后，通常会跟一个激励函数，用于增加非线性因素，如常用的 ReLu函数（`f(x) = max(x, 0)`）。  

通常，卷积核的大小会取 **_奇数_**，因为这样能够有确定的卷积核中心，并且可以避免对输入添加 padding 。  

一个大小为 N x N 的信息矩阵，经过大小为 n x n 的矩阵后，可以提取出一个大小为 (N - n + 1) x (N - n + 1) 的特征信息矩阵。  

# 3. Padding 和 Stride

- Padding 补白，用于在信息矩阵周边补充空白信息，通常补充 0 。这样能够在进行卷积的时候避免一些信息被遗漏。 

- Stride，步长，定义每次移动多少个距离来取信息矩阵块。  


# 4. 池化层
池化层是对卷积后得到的 Feature Map 进行池化操作，目的是为了再一次抽象特征，顺便压缩。  

通常，取 stride = 1，能够尽可能多的提取到输入信息的特征，但会包含很多无关紧要的特征，利用池化可以将这些无关紧要的信息过滤掉一部分。  

池化通常使用两种方式： 

**1. Max pooling 即最大化，取范围内的最大值。**  

![](https://raw.githubusercontent.com/chenBingX/img/master/机器学习相关/池化示意图.png)

如图的一个 Feature Map ，经过一个 stride = 2 的 Max Pooling Filter 滤波器池化后，得到右边的结果矩阵。这个结果会被保存在 Max Pooling 层中。

**2. Mean pooling，即平均化，计算范围内数值的平均值。**  

![](https://raw.githubusercontent.com/chenBingX/img/master/机器学习相关/average池化示意图.png)
  
  
在使用CNN处理图像的过程中，经过卷积或者池化后，图像尺寸缩放计算规则如下：  

- 如果 padding 选型为 “SAME”，输出尺寸为：  

![$\frac{图像尺寸}{步长}$](http://ogemdlrap.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-10%20%E4%B8%8B%E5%8D%887.11.24.png)

- 如果 padding 选型为 “VALID”，输出尺寸为： 


![$\frac{图像尺寸 - 核尺寸 + 1}{步长}$](http://ogemdlrap.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-10%20%E4%B8%8B%E5%8D%887.12.53.png)



# 5. 例子

## 5.1 VGG-16网络

VGG-16 全称 Visual Geometry Group，是一个牛津大学的视觉科研组开发出来的卷积神经网络，它是一个有16层参数层的神经网络。  

![](http://ogemdlrap.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-02%20%E4%B8%8B%E5%8D%886.13.07.png)  


## 5.2 GoogleNet网络

这是一个深度达22层的卷积神经网络，它会将输入展开用多个卷积核并行处理。  

![](http://ogemdlrap.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202018-10-02%20%E4%B8%8B%E5%8D%886.17.45.png)  




# 6. 后记

卷积神经网络，通过让卷积核滑动，逐一地提取输入向量中的特征，然后经过池化去除噪声，使得卷积出来的特征具有一定的泛化性，即在进行识别时，能够自动的忽略掉一些不重要的信息。这个过程就像像是人类的记忆过程，我们记住一些特征，帮助我们下次看到类似的事物时，能够辨别出它们来。

